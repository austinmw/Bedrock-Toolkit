{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedrock LLM Agent Tool Use Framework\n",
    "\n",
    "## Background\n",
    "\n",
    "In the rapidly evolving landscape of Large Language Model (LLM) applications, developers often encounter unexpected challenges when integrating these models with custom tools and workflows. A significant issue that has emerged, particularly with Anthropic models, is the lack of server-side schema verification for tool responses. This means that, contrary to what some developers might expect, there's no guarantee that the LLM's responses will strictly adhere to the input tool specifications.\n",
    "\n",
    "This discrepancy can lead to substantial problems, especially when these potentially non-conforming responses are used as inputs for downstream functions or processes. The issue is further complicated when working with popular LLM orchestration frameworks like LangChain.\n",
    "\n",
    "LangChain and similar frameworks have introduced features like `bind_tools` to support Pydantic models. Their approach typically involves:\n",
    "\n",
    "1. Converting Pydantic models to OpenAI-format tool specifications\n",
    "2. Sending these specifications to the model as input\n",
    "3. Receiving a JSON response from the model\n",
    "4. Attempting to parse this response back into a filled-out Pydantic object\n",
    "\n",
    "However, due to the lack of guaranteed schema conformity in the LLM's output, the final parsing step can fail. Within these frameworks, it's often challenging to implement robust client-side verification and effective retry logic at this critical point of potential failure.\n",
    "\n",
    "This framework was created to address these specific challenges, offering a more flexible and resilient approach to LLM-tool integration, particularly within the AWS Bedrock ecosystem.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This framework provides a robust solution for integrating large language models (LLMs) with custom tools, specifically designed for use with AWS Bedrock. It offers a streamlined approach to handling complex, multi-turn conversations that require external data processing or computation. The framework leverages Pydantic for defining tool interfaces, allowing for flexible output options where Pydantic objects can serve as the final desired output or as validated inputs for downstream tools and APIs.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Pydantic-based Tool Definitions**: Leverage the power of Pydantic for type-safe and validated tool interfaces.\n",
    "- **Automatic ToolSpec Conversion**: Seamlessly converts Pydantic models to Bedrock-compatible toolSpecs.\n",
    "- **Multi-turn Interaction Support**: Handles complex conversation flows with multiple tool calls.\n",
    "- **Response Parsing**: Ensures LLM outputs adhere to expected schemas.\n",
    "- **Sophisticated Error Handling**: Implements retry logic with LLM feedback for improved robustness.\n",
    "- **Flexible Output Options**: Return Pydantic objects for direct use or further processing, or raw assistant messages.\n",
    "- **Streaming Support**: Enable real-time interaction where needed.\n",
    "- **Multi-tool Scenario Handling**: Manage conversations requiring multiple different tools.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. Define your tools using Pydantic models.\n",
    "2. The framework converts these models to Bedrock toolSpecs using `pydantic_to_toolspec()`.\n",
    "3. Engage in multi-turn conversations with the LLM, automatically calling tools as needed.\n",
    "4. Parse and validate LLM responses to ensure schema compliance.\n",
    "5. Handle errors gracefully, providing feedback to the LLM for potential self-correction.\n",
    "6. Process tool outputs, either returning Pydantic objects directly for use as final output or as validated input for downstream processes, or perform additional computations as needed.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- `register_models_from_tools()`: Creates a global registry of all Pydantic models, including nested ones.\n",
    "- `pydantic_to_toolspec()`: Converts Pydantic models to LLM-compatible tool specifications.\n",
    "- `generate_text()`: Manages LLM interactions with retry logic.\n",
    "- `process_tool_use()`: Parses LLM tool responses into Pydantic objects, and passes these as inputs to corresponding tool processor functions, returning the output to the model.\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "# Example usage (simplified)\n",
    "tool_schemas = [WeatherTool, CityInfoTool]\n",
    "\n",
    "response_dict = call_model_with_tools(\n",
    "    model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    # Define an initial input prompt\n",
    "    user_prompt=\"What's the weather like in New York?\",\n",
    "    # List the Pydantic models to use\n",
    "    tool_schemas=tool_schemas,\n",
    "    # Optional: List of functions to process pydantic outputs\n",
    "    # If None, the Pydantic object itself will be the tool output\n",
    "    tool_processors=None,\n",
    "    use_streaming=True,\n",
    "    invoke_limit=None,\n",
    "    max_retries=3,\n",
    "    log_level=\"INFO\",\n",
    ")\n",
    "\n",
    "for tool_output in response_dict[\"tool_calls\"]:\n",
    "    if isinstance(tool_output, WeatherInfo):\n",
    "        # Access the properties of the WeatherInfo object directly\n",
    "        print(f\"Temperature: {response.temperature}Â°C\")\n",
    "```\n",
    "\n",
    "## Why This Framework?\n",
    "\n",
    "While similar to other LLM orchestration frameworks like LangChain's `bind_tools`, this solution is tailored specifically for AWS Bedrock. It provides a balance of convenience (through Pydantic usage) and compatibility (with Bedrock's toolSpec requirements), making it an excellent choice for developers working within the AWS ecosystem. The framework's flexibility in handling Pydantic objects as both final outputs and inputs for further processing enhances its utility in complex workflows.\n",
    "\n",
    "## Advanced Features\n",
    "\n",
    "- **Nested Schema Handling**: Supports complex tool definitions with nested Pydantic models.\n",
    "- **Flexible Tool Processing**: Tool functions can return Pydantic model instances for direct use, perform complex operations with external APIs, or serve as validated inputs for downstream processes.\n",
    "- **Comprehensive Logging**: Detailed logging at each step for debugging and monitoring.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. Clone the repository\n",
    "2. Install dependencies: `pip install -r requirements.txt`\n",
    "3. Define your Pydantic models for tools\n",
    "4. Set up your AWS credentials for Bedrock access\n",
    "5. Use the `call_model_with_tools` function to start interacting with your LLM and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U loguru boto3 pydantic rich requests Wikipedia-API\n",
    "\n",
    "# from rich.traceback import install\n",
    "# install(show_locals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- Add way to send streaming to Streamlit\n",
    "- Add control for system prompt\n",
    "- Add `inferenceConfig` to converse API\n",
    "- Add `guardrailConfig` to converse api\n",
    "- Investigate `guardContent` in system message\n",
    "\n",
    "\n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse_stream.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price per 1,000 input tokens\n",
    "global BEDROCK_PRICING\n",
    "BEDROCK_PRICING = {\n",
    "    \"us-east-1\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"anthropic.claude-3-5-sonnet-20240620-v1:0\": {\"input\": 0.003, \"output\": 0.015},\n",
    "        \"anthropic.claude-3-haiku-20240307-v1:0\": {\"input\": 0.00025, \"output\": 0.00125},\n",
    "        \"anthropic.claude-3-sonnet-20240229-v1:0\": {\"input\": 0.003, \"output\": 0.015},\n",
    "        \"anthropic.claude-v2:1\": {\"input\": 0.008, \"output\": 0.024},\n",
    "        \"anthropic.claude-v2\": {\"input\": 0.008, \"output\": 0.024},\n",
    "        \"anthropic.claude-instant-v1\": {\"input\": 0.0008, \"output\": 0.0024},\n",
    "        \"meta.llama2-13b-chat-v1\": {\"input\": 0.00075, \"output\": 0.001},\n",
    "        \"meta.llama2-70b-chat-v1\": {\"input\": 0.00195, \"output\": 0.00256},\n",
    "    },\n",
    "    \"us-west-2\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"anthropic.claude-3-opus-20240229-v1:0\": {\"input\": 0.015, \"output\": 0.075},\n",
    "        \"anthropic.claude-3-haiku-20240307-v1:0\": {\"input\": 0.00025, \"output\": 0.00125},\n",
    "        \"anthropic.claude-3-sonnet-20240229-v1:0\": {\"input\": 0.003, \"output\": 0.015},\n",
    "        \"anthropic.claude-v2:1\": {\"input\": 0.008, \"output\": 0.024},\n",
    "        \"anthropic.claude-v2\": {\"input\": 0.008, \"output\": 0.024},\n",
    "        \"anthropic.claude-instant-v1\": {\"input\": 0.0008, \"output\": 0.0024},\n",
    "        \"meta.llama2-13b-chat-v1\": {\"input\": 0.00075, \"output\": 0.001},\n",
    "        \"meta.llama2-70b-chat-v1\": {\"input\": 0.00195, \"output\": 0.00256},\n",
    "    },\n",
    "    \"eu-west-2\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"anthropic.claude-3-haiku-20240307-v1:0\": {\"input\": 0.00025, \"output\": 0.00125},\n",
    "        \"anthropic.claude-3-sonnet-20240229-v1:0\": {\"input\": 0.003, \"output\": 0.015},\n",
    "    },\n",
    "    \"sa-east-1\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"anthropic.claude-3-haiku-20240307-v1:0\": {\"input\": 0.00025, \"output\": 0.00125},\n",
    "        \"anthropic.claude-3-sonnet-20240229-v1:0\": {\"input\": 0.003, \"output\": 0.015},\n",
    "    },\n",
    "    \"ca-central-1\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"anthropic.claude-3-haiku-20240307-v1:0\": {\"input\": 0.00025, \"output\": 0.00125},\n",
    "        \"anthropic.claude-3-sonnet-20240229-v1:0\": {\"input\": 0.003, \"output\": 0.015},\n",
    "    },\n",
    "    \"ap-south-1\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"anthropic.claude-3-haiku-20240307-v1:0\": {\"input\": 0.00025, \"output\": 0.00125},\n",
    "        \"anthropic.claude-3-sonnet-20240229-v1:0\": {\"input\": 0.003, \"output\": 0.015},\n",
    "    },\n",
    "    \"ap-southeast-2\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"anthropic.claude-3-haiku-20240307-v1:0\": {\"input\": 0.00025, \"output\": 0.00125},\n",
    "        \"anthropic.claude-3-sonnet-20240229-v1:0\": {\"input\": 0.003, \"output\": 0.015},\n",
    "    },\n",
    "    \"eu-west-3\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"anthropic.claude-3-haiku-20240307-v1:0\": {\"input\": 0.00025, \"output\": 0.00125},\n",
    "        \"anthropic.claude-3-sonnet-20240229-v1:0\": {\"input\": 0.003, \"output\": 0.015},\n",
    "    },\n",
    "    \"eu-central-1\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"anthropic.claude-v2:1\": {\"input\": 0.008, \"output\": 0.024},\n",
    "        \"anthropic.claude-v2\": {\"input\": 0.008, \"output\": 0.024},\n",
    "        \"anthropic.claude-instant-v1\": {\"input\": 0.0008, \"output\": 0.0024},\n",
    "    },\n",
    "    \"ap-northeast-1\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"anthropic.claude-v2:1\": {\"input\": 0.008, \"output\": 0.024},\n",
    "        \"anthropic.claude-v2\": {\"input\": 0.008, \"output\": 0.024},\n",
    "        \"anthropic.claude-instant-v1\": {\"input\": 0.0008, \"output\": 0.0024},\n",
    "    },\n",
    "    \"ap-east-1\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "    },\n",
    "    \"ap-southeast-1\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "    },\n",
    "    \"ap-southeast-3\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "    },\n",
    "    \"eu-north-1\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "    },\n",
    "    \"eu-west-1\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "    },\n",
    "    \"me-south-1\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "    },\n",
    "    \"us-east-2\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "    },\n",
    "    \"us-west-1\": {\n",
    "        \"ai21.j2-mid-v1\": {\"input\": 0.0125, \"output\": 0.0125},\n",
    "        \"ai21.j2-ultra-v1\": {\"input\": 0.0188, \"output\": 0.0188},\n",
    "        \"ai21.jamba-instruct-v1:0\": {\"input\": 0.0005, \"output\": 0.0007},\n",
    "        \"cohere.command-text-v14\": {\"input\": 0.0015, \"output\": 0.0020},\n",
    "        \"cohere.command-light-text-v14\": {\"input\": 0.0003, \"output\": 0.0006},\n",
    "        \"cohere.command-r-plus-v1:0\": {\"input\": 0.0030, \"output\": 0.0150},\n",
    "        \"cohere.command-r-v1:0\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"cohere.embed-english-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "        \"cohere.embed-multilingual-v3\": {\"input\": 0.0001, \"output\": None},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from loguru import logger\n",
    "import time\n",
    "import json\n",
    "import inspect\n",
    "from functools import wraps\n",
    "from typing import Any, List, Literal, Type, Generator, Callable\n",
    "from pydantic import BaseModel, Field\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "from rich.console import Console\n",
    "from rich.logging import RichHandler\n",
    "from rich.status import Status\n",
    "from contextlib import contextmanager\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "LogLevel = Literal[\"TRACE\", \"DEBUG\", \"INFO\", \"SUCCESS\", \"WARNING\", \"ERROR\", \"CRITICAL\"]\n",
    "\n",
    "def pydantic_to_toolspec(model: type[BaseModel]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert a Pydantic model to a toolSpec dictionary, including additional field arguments.\n",
    "\n",
    "    This function takes a Pydantic model and converts it into a toolSpec format,\n",
    "    which includes the model's name, description (using the model's docstring if available),\n",
    "    and a JSON schema of its structure, including additional constraints like ge, min_items, and max_items.\n",
    "\n",
    "    Args:\n",
    "        model: A Pydantic model class.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary representing the toolSpec for the input model.\n",
    "    \"\"\"\n",
    "    def convert_schema(schema: dict[str, Any], defs: dict[str, Any]) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Recursively convert a JSON schema dictionary, resolving any references\n",
    "        and including additional field arguments.\n",
    "\n",
    "        Args:\n",
    "            schema: The schema dictionary to convert.\n",
    "            defs: The definitions dictionary for resolving references.\n",
    "\n",
    "        Returns:\n",
    "            The converted schema dictionary.\n",
    "        \"\"\"\n",
    "        if \"$ref\" in schema:\n",
    "            ref_key = schema[\"$ref\"].split(\"/\")[-1]\n",
    "            return convert_schema(defs[ref_key], defs)\n",
    "\n",
    "        result: dict[str, Any] = {}\n",
    "        for key in [\"type\", \"description\"]:\n",
    "            if key in schema:\n",
    "                result[key] = schema[key]\n",
    "\n",
    "        # Include additional constraints\n",
    "        for constraint in [\"minimum\", \"maximum\", \"exclusiveMinimum\", \"exclusiveMaximum\", \"minItems\", \"maxItems\"]:\n",
    "            if constraint in schema:\n",
    "                result[constraint] = schema[constraint]\n",
    "\n",
    "        if \"properties\" in schema:\n",
    "            result[\"properties\"] = {\n",
    "                k: convert_schema(v, defs) for k, v in schema[\"properties\"].items()\n",
    "            }\n",
    "\n",
    "        if \"items\" in schema:\n",
    "            result[\"items\"] = convert_schema(schema[\"items\"], defs)\n",
    "\n",
    "        if \"required\" in schema:\n",
    "            result[\"required\"] = schema[\"required\"]\n",
    "\n",
    "        return result\n",
    "\n",
    "    schema = model.model_json_schema()\n",
    "    defs = schema.get(\"$defs\", {})\n",
    "\n",
    "    # Use the model's docstring if it exists, otherwise fall back to the schema description or a default\n",
    "    description = model.__doc__.strip() if model.__doc__ else schema.get(\"description\", f\"Model for {schema['title']}\")\n",
    "\n",
    "    return {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": schema[\"title\"],\n",
    "            \"description\": description,\n",
    "            \"inputSchema\": {\n",
    "                \"json\": convert_schema(schema, defs)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def format_tools(models: list[type[BaseModel]]) -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Convert multiple Pydantic models to a list of toolSpec dictionaries.\n",
    "\n",
    "    Args:\n",
    "        models: A list of Pydantic model classes.\n",
    "\n",
    "    Returns:\n",
    "        A list of toolSpec dictionaries, one for each input model.\n",
    "    \"\"\"\n",
    "    formatted_tools = [pydantic_to_toolspec(model) for model in models]\n",
    "    for n, formatted_tool in enumerate(formatted_tools):\n",
    "        logger.debug(f\"Tool {n + 1}:\\n{json.dumps(formatted_tool, indent=4)}\")\n",
    "    return formatted_tools\n",
    "\n",
    "\n",
    "def register_models_from_tools(tool_schemas: List[Type[BaseModel]]):\n",
    "    \"\"\"Registers all Pydantic models from the tools list, including nested models.\"\"\"\n",
    "    pydantic_model_registry: dict[str, Type[BaseModel]] = {}\n",
    "\n",
    "    def register_pydantic_model(model: Type[BaseModel]):\n",
    "        \"\"\"Registers a Pydantic model in the global registry.\"\"\"\n",
    "        pydantic_model_registry[model.__name__] = model\n",
    "\n",
    "\n",
    "    def register_nested_models(model: Type[BaseModel], visited: set):\n",
    "        if model in visited:\n",
    "            return\n",
    "        visited.add(model)\n",
    "        register_pydantic_model(model)\n",
    "        for field in model.model_fields.values():\n",
    "            field_type = field.annotation\n",
    "            if inspect.isclass(field_type) and issubclass(field_type, BaseModel):\n",
    "                register_nested_models(field_type, visited)\n",
    "            elif (getattr(field_type, '__origin__', None) is list and\n",
    "                  inspect.isclass(field_type.__args__[0]) and issubclass(field_type.__args__[0], BaseModel)):\n",
    "                register_nested_models(field_type.__args__[0], visited)\n",
    "\n",
    "    visited_models = set()\n",
    "    for model in tool_schemas:\n",
    "        register_nested_models(model, visited_models)\n",
    "\n",
    "    return pydantic_model_registry\n",
    "\n",
    "\n",
    "def stream_messages(bedrock_client, model_id: str, messages: list[dict[str, Any]], system_prompt: dict, tool_config: dict[str, Any]) -> Generator[str, None, tuple[dict, str, dict[str, Any], Counter]]:\n",
    "    \"\"\"Streams messages to a model and processes the response.\"\"\"\n",
    "    logger.debug(f\"Streaming messages with model {model_id}\")\n",
    "\n",
    "    response = bedrock_client.converse_stream(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=[system_prompt],\n",
    "        toolConfig=tool_config\n",
    "    )\n",
    "\n",
    "    stop_reason = \"\"\n",
    "    message = {\"content\": []}\n",
    "    text = ''\n",
    "    tool_use = {}\n",
    "    stream_usage = Counter()\n",
    "\n",
    "    try:\n",
    "        for chunk in response['stream']:\n",
    "            if 'metadata' in chunk:\n",
    "                stream_usage.update(chunk['metadata']['usage'])\n",
    "            if 'messageStart' in chunk:\n",
    "                message['role'] = chunk['messageStart']['role']\n",
    "            elif 'contentBlockStart' in chunk:\n",
    "                tool = chunk['contentBlockStart']['start']['toolUse']\n",
    "                tool_use = {'toolUseId': tool['toolUseId'], 'name': tool['name']}\n",
    "            elif 'contentBlockDelta' in chunk:\n",
    "                delta = chunk['contentBlockDelta']['delta']\n",
    "                if 'toolUse' in delta:\n",
    "                    tool_use['input'] = tool_use.get('input', '') + delta['toolUse']['input']\n",
    "                elif 'text' in delta:\n",
    "                    text += delta['text']\n",
    "                    yield delta['text']  # Yield the text chunk\n",
    "            elif 'contentBlockStop' in chunk:\n",
    "                if 'input' in tool_use:\n",
    "                    tool_use['input'] = json.loads(tool_use['input'])\n",
    "                    message['content'].append({'toolUse': tool_use})\n",
    "                    tool_use = {}\n",
    "                else:\n",
    "                    message['content'].append({'text': text})\n",
    "                    text = ''\n",
    "            elif 'messageStop' in chunk:\n",
    "                stop_reason = chunk['messageStop']['stopReason']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during streaming: {str(e)}\")\n",
    "    finally:\n",
    "        # Ensure we always return the values, even if an exception occurred\n",
    "        yield response, stop_reason, message, stream_usage\n",
    "\n",
    "\n",
    "\n",
    "def parse_model(tool: dict[str, Any]) -> BaseModel:\n",
    "    \"\"\"Parses a tool dictionary into the appropriate Pydantic model.\"\"\"\n",
    "    global pydantic_model_registry\n",
    "\n",
    "    if not isinstance(tool, dict):\n",
    "        raise ValueError(\"Input tool must be a dictionary.\")\n",
    "\n",
    "    model_name = tool.get('name')\n",
    "    input_data = tool.get('input', {})\n",
    "\n",
    "    if model_name is None or model_name not in pydantic_model_registry:\n",
    "        raise ValueError(\"Invalid or missing 'name' in input tool.\")\n",
    "\n",
    "    model_class = pydantic_model_registry[model_name]\n",
    "    return model_class.model_validate(input_data)\n",
    "\n",
    "\n",
    "def process_tool_use(tool: dict[str, Any], messages: list[dict[str, Any]], tool_processors: dict | None) -> None:\n",
    "    \"\"\"Processes the tool use request.\"\"\"\n",
    "\n",
    "    def default_process_function(parsed_model: BaseModel) -> dict[str, Any]:\n",
    "        \"\"\"Default process function for tools.\"\"\"\n",
    "        return parsed_model.model_dump()\n",
    "\n",
    "    try:\n",
    "        # Parse the tool input into the appropriate Pydantic model\n",
    "        tool_call = parse_model(tool)\n",
    "\n",
    "        # Get the tool function from the tool_functions dictionary, or use the default function\n",
    "        if tool_processors is None:\n",
    "            tool_function = default_process_function\n",
    "        else:\n",
    "            tool_function = tool_processors.get(tool[\"name\"], default_process_function)\n",
    "\n",
    "        # Process the parsed Pydantic object and get the tool output\n",
    "        # This could just be a simple model_dump() call to convert the Pydantic object serializable object,\n",
    "        # Or it could be a more complex function that uses the Pydantic object in some way.\n",
    "        tool_output = tool_function(tool_call)\n",
    "\n",
    "        tool_result = {\n",
    "            \"toolUseId\": tool['toolUseId'],\n",
    "            \"content\": [{\"json\": tool_output}]\n",
    "        }\n",
    "\n",
    "        tool_result_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"toolResult\": tool_result}]\n",
    "        }\n",
    "\n",
    "        logger.debug(f\"Tool result: {json.dumps(tool_result_message, indent=4)}\")\n",
    "\n",
    "        messages.append(tool_result_message)\n",
    "\n",
    "        return tool_call\n",
    "\n",
    "    except Exception as err:\n",
    "        error_message = (\n",
    "            \"Pydantic model validation failed. The LLM failed to return a valid response.\\n\"\n",
    "            f\"Validation error details:\\n{str(err)}\\n\\n\"\n",
    "            f\"Tool response that caused this error:\\n{json.dumps(tool['input'], indent=4)}\\n\\n\"\n",
    "        )\n",
    "        logger.error(error_message)\n",
    "\n",
    "        raise  # Re-raise the exception to trigger retry\n",
    "\n",
    "\n",
    "def generate_text(region: str, model_id: str, messages: list[dict[str, Any]], tool_config: dict[str, Any], tool_processors: dict | None, use_streaming: bool, invoke_limit: int | None = None, max_retries: int = 3) -> Generator[str | dict[str, Any], None, None]:\n",
    "    \"\"\"Generates text using the supplied Amazon Bedrock model with built-in retry logic.\"\"\"\n",
    "\n",
    "    original_messages = messages.copy()\n",
    "    original_user_message = original_messages[-1]['content'][0]['text']\n",
    "    error_message_pairs = []\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            yield from _generate_text_core(region, model_id, messages, tool_config, tool_processors, use_streaming, invoke_limit)\n",
    "            return\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(\n",
    "                f\"Attempt {attempt + 1} failed. Exception:\\n{str(e)}\\n\\n\"\n",
    "                \"Note: If you see failures and retry attempts often, you should improve the Pydantic model definitions and docstring.\"\n",
    "            )\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "\n",
    "            # Extract the assistant's last failed response's 'input' parts\n",
    "            last_assistant_message = messages[-1] if messages[-1]['role'] == 'assistant' else None\n",
    "            tool_inputs = []\n",
    "\n",
    "            if last_assistant_message:\n",
    "                for content in last_assistant_message['content']:\n",
    "                    if 'toolUse' in content:\n",
    "                        tool_inputs.append(json.dumps(content['toolUse']['input'], indent=4))\n",
    "\n",
    "            # Add the current error-message pair to the list, including only the 'input' parts\n",
    "            error_message_pairs.append((str(e), tool_inputs))\n",
    "\n",
    "            # Reset messages to original state\n",
    "            messages = original_messages.copy()\n",
    "\n",
    "            # Create error_context with all previous error-message pairs\n",
    "            error_context = \"\\n\\nNote: Previous attempts resulted in errors. Here's a summary:\\n\"\n",
    "            for i, (err, inputs) in enumerate(error_message_pairs, 1):\n",
    "                error_context += f\"\\nAttempt {i} error:\\n{err}\\n\"\n",
    "                for input_msg in inputs:\n",
    "                    error_context += f\"The tool call(s) that caused this error was:\\n{input_msg}\\n\"\n",
    "\n",
    "            error_context += (\n",
    "                \"\\n\\nPlease reflect on the provided schema, your previous responses, and these error messages. \"\n",
    "                \"Now try again, ensuring that your tool use response matches the input schema exactly! \"\n",
    "                \"Critically important: You will lose points if you fail to provide a syntactically correct response! \"\n",
    "                \"Do not output the same incorrect schema again. \"\n",
    "                \"Also, before returning a tool, acknowledge the error in your text response, \"\n",
    "                \"explain why you think it occurred, and describe how you plan to address it. \"\n",
    "                \"Remember that a missing parameter error may be due to the parameter actually missing, \"\n",
    "                \"or it could be due to the parameter being present but not in the expected location. \"\n",
    "                \"Finally, you should not include a top-level 'properties' key in tool responses.\"\n",
    "            )\n",
    "\n",
    "            # Update the last user message with cumulative error information\n",
    "            messages[-1]['content'][0]['text'] = original_user_message + error_context\n",
    "\n",
    "    raise Exception(f\"All {max_retries} attempts failed\")\n",
    "\n",
    "\n",
    "def _generate_text_core(region: str, model_id: str, messages: list[dict[str, Any]], tool_config: dict[str, Any], tool_processors: dict | None, use_streaming: bool, invoke_limit: int | None) -> Generator[str | dict[str, Any], None, None]:\n",
    "    \"\"\"Core logic for generating text using the Amazon Bedrock model.\"\"\"\n",
    "    # Define custom configuration\n",
    "    config = Config(\n",
    "        retries={\n",
    "            'max_attempts': 10,  # Number of retry attempts\n",
    "            'mode': 'adaptive'   # Retry mode (standard or adaptive)\n",
    "        },\n",
    "        read_timeout=120,  # Read timeout in seconds\n",
    "        connect_timeout=30  # Connect timeout in seconds\n",
    "    )\n",
    "\n",
    "    # Create the bedrock client with the custom configuration\n",
    "    bedrock_client = boto3.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        config=config,\n",
    "        region_name=region,\n",
    "    )\n",
    "\n",
    "    stop_reason = 'tool_use'\n",
    "    first_tool_use = True\n",
    "    tool_calls = []\n",
    "    invoke_count = 0\n",
    "\n",
    "    system_prompt = {\n",
    "        \"text\": (\n",
    "                    \"You are an intelligent assistant capable of calling functions to gather all necessary information to answer a user's question comprehensively. \"\n",
    "                    \"Each response should build upon the previous one, combining all gathered information into a final, standalone answer. \"\n",
    "                    \"Ensure the final response fully addresses the user's question, presenting it as if you are answering from scratch, \"\n",
    "                    \"without assuming the user remembers any information from previous responses. \"\n",
    "                    \"Provide the final response directly, without any introductory or transitional phrases.\"\n",
    "                    \"Finally, you should not include a top-level 'properties' key in tool responses.\"\n",
    "                )\n",
    "    }\n",
    "\n",
    "    usage = Counter()\n",
    "\n",
    "    while stop_reason == 'tool_use':\n",
    "        current_tool_config = tool_config if first_tool_use else {\n",
    "            \"tools\": tool_config[\"tools\"],\n",
    "            \"toolChoice\": {\"auto\": {}}\n",
    "        }\n",
    "        first_tool_use = False\n",
    "\n",
    "        logger.info(f\"Generating text with model {model_id}\")\n",
    "\n",
    "        logger.debug(\"Messages:\")\n",
    "        for message in messages:\n",
    "            role = message[\"role\"]\n",
    "            context = message[\"content\"]\n",
    "            logger.debug(f\"{role}:\")\n",
    "            for part in context:\n",
    "                if 'text' in part:\n",
    "                    logger.debug(f\"text:\\n{part['text']}\")\n",
    "                elif 'toolUse' in part:\n",
    "                    logger.debug(f\"toolUse:\\n{json.dumps(part['toolUse'], indent=4)}\")\n",
    "                elif 'toolResult' in part:\n",
    "                    logger.debug(f\"toolResult:\\n{json.dumps(part['toolResult'], indent=4)}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected message part: {part}\")\n",
    "\n",
    "        logger.debug(\"Sending messages to model...\")\n",
    "\n",
    "        if use_streaming:\n",
    "            generator = stream_messages(bedrock_client, model_id, messages, system_prompt, current_tool_config)\n",
    "\n",
    "            for item in generator:\n",
    "                if isinstance(item, str):\n",
    "                    yield item  # Yield the text chunk\n",
    "                else:\n",
    "                    # This is the final yield with the return values\n",
    "                    response, stop_reason, message, stream_usage = item\n",
    "\n",
    "            logger.debug(f\"Bedrock response (streamed):\\n{response}\")\n",
    "            usage.update(stream_usage)\n",
    "\n",
    "        else:\n",
    "            response = bedrock_client.converse(\n",
    "                modelId=model_id,\n",
    "                messages=messages,\n",
    "                system=[system_prompt],\n",
    "                inferenceConfig={\"maxTokens\": 4096, \"temperature\": 0},\n",
    "                toolConfig=current_tool_config\n",
    "            )\n",
    "            logger.debug(f\"Bedrock response:\\n{json.dumps(response, indent=4)}\")\n",
    "\n",
    "            message = response['output']['message']\n",
    "            stop_reason = response['stopReason']\n",
    "            usage.update(response[\"usage\"])\n",
    "\n",
    "        messages.append(message)\n",
    "\n",
    "        if stop_reason == 'tool_use':\n",
    "            for content in message['content']:\n",
    "                if 'toolUse' in content:\n",
    "                    tool = content['toolUse']\n",
    "                    logger.debug(f\"Requesting tool {tool['name']}. Request: {tool['toolUseId']}. Inputs: {tool['input']}\")\n",
    "                    tool_call = process_tool_use(tool, messages, tool_processors)\n",
    "                    tool_calls.append(tool_call)\n",
    "\n",
    "        invoke_count += 1\n",
    "\n",
    "        if invoke_limit is not None and invoke_count >= invoke_limit:\n",
    "            logger.info(f\"Reached invoke limit of {invoke_limit}. Stopping generation.\")\n",
    "            break\n",
    "\n",
    "    if not use_streaming:\n",
    "        for content in message['content']:\n",
    "            if 'text' in content:\n",
    "                yield content['text']\n",
    "\n",
    "    logger.info(f\"Total token usage:\\n{json.dumps(usage, indent=2)}\")\n",
    "\n",
    "    global BEDROCK_PRICING\n",
    "    model_pricing = BEDROCK_PRICING[region][model_id]\n",
    "    input_tokens = usage[\"inputTokens\"]\n",
    "    output_tokens = usage[\"outputTokens\"]\n",
    "    total_price = (input_tokens / 1000) * model_pricing[\"input\"] + (output_tokens / 1000) * model_pricing[\"output\"]\n",
    "    total_price_str = f\"${total_price:.3f}\"\n",
    "    usage[\"totalPrice\"] = total_price_str\n",
    "    logger.info(f\"Total price: {total_price_str}\")\n",
    "\n",
    "    response_dict = {\n",
    "        \"messages\": messages,\n",
    "        \"tool_calls\": tool_calls,\n",
    "        \"usage\": usage\n",
    "    }\n",
    "\n",
    "    yield response_dict\n",
    "\n",
    "@contextmanager\n",
    "def spinning_logger(console, message: str):\n",
    "    \"\"\"\n",
    "    A context manager that displays an animated spinner with a message while a task is in progress.\n",
    "    \"\"\"\n",
    "    with console.status(message, spinner=\"dots\") as status:\n",
    "        try:\n",
    "            yield status\n",
    "        finally:\n",
    "            status.stop()\n",
    "\n",
    "\n",
    "def call_model_with_tools(\n",
    "    user_prompt: str,\n",
    "    model_id: str,\n",
    "    tool_schemas: list,\n",
    "    tool_processors: dict | None,\n",
    "    log_level: LogLevel = \"INFO\",\n",
    "    use_streaming: bool = False,\n",
    "    invoke_limit: int | None = None,\n",
    "    max_retries: bool = 3,\n",
    ") -> Generator[str | dict[str, Any], None, None]:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Configure loguru with Rich using the passed log_level\n",
    "    # Create a custom Console with increased width\n",
    "    console = Console(width=160)  # Adjust the width as needed\n",
    "    logger.remove()  # Remove any existing handlers\n",
    "    logger.add(\n",
    "        RichHandler(console=console, markup=False),\n",
    "        level=log_level,\n",
    "        format=\"<level>{level}</level>: <level>{message}</level>\"\n",
    "    )\n",
    "\n",
    "    #with spinning_logger(console, \"Calling model with tools...\") as status:\n",
    "\n",
    "    try:\n",
    "        # Create a list of messages with the user's question as the first message\n",
    "        messages = [{\"role\": \"user\", \"content\": [{\"text\": user_prompt}]}]\n",
    "\n",
    "        # Register the Pydantic models from the tools list\n",
    "        global pydantic_model_registry\n",
    "        pydantic_model_registry = register_models_from_tools(tool_schemas)\n",
    "\n",
    "        # Format the tools for the model\n",
    "        tool_config = {\"tools\": format_tools(tool_schemas)}\n",
    "\n",
    "        # Generate text using the model\n",
    "        response_generator = generate_text(\n",
    "            region=\"us-east-1\",\n",
    "            model_id=model_id,\n",
    "            messages=messages,\n",
    "            tool_config=tool_config,\n",
    "            tool_processors=tool_processors,\n",
    "            use_streaming=use_streaming,\n",
    "            invoke_limit=invoke_limit,\n",
    "            max_retries=max_retries,\n",
    "        )\n",
    "\n",
    "        # Yield all items from the generator\n",
    "        yield from response_generator\n",
    "\n",
    "    except Exception as err:\n",
    "        logger.exception(f\"Error calling model with tools.\")\n",
    "        yield {\"error\": str(err)}\n",
    "    finally:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Elapsed time: {elapsed_time:.3f} seconds\")\n",
    "        logger.debug(f\"Finished generating response with model {model_id}.\")\n",
    "\n",
    "\n",
    "\n",
    "def process_generator_output(\n",
    "    generator: Generator[str | dict[str, Any], None, None],\n",
    "    stream_callback: Callable[[str], None] = lambda x: print(x, end='', flush=True),\n",
    "    error_callback: Callable[[str], None] = lambda x: print(f\"Error occurred: {x}\"),\n",
    ") -> tuple[str, dict[str, Any] | None]:\n",
    "    \"\"\"\n",
    "    Process the output from the response generator.\n",
    "\n",
    "    Args:\n",
    "    generator: The generator yielding strings (for streaming) and dicts (for final response)\n",
    "    stream_callback: Function to call with each text chunk (default: print to console)\n",
    "    error_callback: Function to call if an error occurs (default: print to console)\n",
    "\n",
    "    Returns:\n",
    "    Tuple containing the complete streamed text and the final response dict (or None if no dict received)\n",
    "    \"\"\"\n",
    "    streamed_text = \"\"\n",
    "    response_data = None\n",
    "\n",
    "    for item in generator:\n",
    "        if isinstance(item, str):\n",
    "            streamed_text += item\n",
    "            stream_callback(item)\n",
    "        elif isinstance(item, dict):\n",
    "            if \"error\" in item:\n",
    "                error_callback(item[\"error\"])\n",
    "            else:\n",
    "                response_data = item\n",
    "\n",
    "    return streamed_text, response_data\n",
    "\n",
    "\n",
    "def print_response_data(\n",
    "    response_data: dict[str, Any],\n",
    "    print_func: Callable[[str], None] = print,\n",
    "    json_indent: int = 2\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Print the details of the response data.\n",
    "\n",
    "    Args:\n",
    "    response_data: The dictionary containing response data including tool calls and usage statistics.\n",
    "    print_func: The function to use for printing (default is the built-in print function).\n",
    "    json_indent: The indentation level for JSON output (default is 2 spaces).\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if not response_data:\n",
    "        print_func(\"No response data received.\")\n",
    "        return\n",
    "\n",
    "    print_func(\"\\n--- Response Data Details ---\")\n",
    "\n",
    "    if \"tool_calls\" in response_data:\n",
    "        print_func(\"\\nTool Calls:\")\n",
    "        for tool_call in response_data[\"tool_calls\"]:\n",
    "            if isinstance(tool_call, BaseModel):\n",
    "                print_func(f\"{tool_call.__class__.__name__}: {tool_call.model_dump_json(indent=json_indent)}\")\n",
    "            else:\n",
    "                print_func(f\"Unknown tool call type: {type(tool_call)}\")\n",
    "\n",
    "    if \"usage\" in response_data:\n",
    "        print_func(\"\\nUsage Statistics:\")\n",
    "        print_func(json.dumps(response_data[\"usage\"], indent=json_indent))\n",
    "\n",
    "    if \"messages\" in response_data:\n",
    "        print_func(\"\\nMessages:\")\n",
    "        for message in response_data[\"messages\"]:\n",
    "            print_func(f\"Role: {message['role']}\")\n",
    "            for content in message['content']:\n",
    "                if 'text' in content:\n",
    "                    print_func(f\"Text: {content['text']}\")\n",
    "                elif 'toolUse' in content:\n",
    "                    print_func(f\"Tool Use: {json.dumps(content['toolUse'], indent=json_indent)}\")\n",
    "                elif 'toolResult' in content:\n",
    "                    print_func(f\"Tool Result: {json.dumps(content['toolResult'], indent=json_indent)}\")\n",
    "            print()\n",
    "    # Add any other relevant sections of response_data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather and City Information Example (Mock APIs)\n",
    "\n",
    "This example demonstrates how the assistant uses two mock API tools:\n",
    "\n",
    "1. WeatherRequest: Simulates getting current weather for a city and country.\n",
    "2. CityInfoRequest: Simulates retrieving background information about a location.\n",
    "\n",
    "The assistant must determine which tools to use based on the user's query and combine the mock data into a coherent response. This example uses predefined mock data to simulate API responses.\n",
    "\n",
    "The interaction occurs in three distinct generations:\n",
    "\n",
    "1. First Generation (Weather Tool Call):\n",
    "   The assistant recognizes the need for weather information. It generates a response containing two components:\n",
    "   a) Text explaining the need to use a weather tool to answer the query.\n",
    "   b) A tool call in JSON format for the WeatherRequest tool.\n",
    "   \n",
    "   This JSON tool call is then parsed back into a corresponding Pydantic object to validate its structure. If valid, it's used as input for a `tool_processor` function. This function may perform additional operations (like making an API call) or simply return the structured output if that's sufficient to answer the question. The result is then appended as a new user message.\n",
    "\n",
    "2. Second Generation (City Info Tool Call):\n",
    "   After receiving the weather data, the assistant recognizes that it needs additional background information about the location. It again generates a response with two components:\n",
    "   a) Text explaining the need for more information about the city.\n",
    "   b) A tool call in JSON format for the CityInfoRequest tool.\n",
    "   \n",
    "   Similarly, this JSON is parsed into a Pydantic object, validated, and processed by its corresponding `tool_processor` function. The result is appended to the messages list.\n",
    "\n",
    "3. Third Generation (Final Answer):\n",
    "   With both weather and city information now available, the assistant generates a final text response that combines all the gathered data into a comprehensive answer to the user's original query. This response does not include a tool call.\n",
    "\n",
    "This multi-turn process allows the assistant to gather all required information step-by-step before providing a complete answer. Users should observe these three separate generations. The first two generations each include both explanatory text and a tool call in JSON format (which is then parsed and processed), while the final generation is a text-only response.\n",
    "\n",
    "The parsing of JSON tool calls back into Pydantic objects serves multiple purposes:\n",
    "1. It validates the structure of the LLM's output, ensuring it conforms to the expected schema.\n",
    "2. It provides type safety and autocompletion for developers working with the tool inputs.\n",
    "3. It allows for easy integration with `tool_processor` functions, which can be designed to work with these typed objects.\n",
    "\n",
    "The `tool_processor` functions offer flexibility in how tool calls are handled. They can perform complex operations like API calls, database queries, or computations. Alternatively, if the structured Pydantic object itself contains all necessary information to answer the query, the `tool_processor` might simply return this object, allowing the LLM to use its structured data in formulating the final response.\n",
    "\n",
    "This approach combines the power of LLM-generated queries with the safety and utility of strongly-typed data structures, enabling robust and flexible tool use in AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    \"\"\" Main function to demonstrate calling a model with tools. \"\"\"\n",
    "\n",
    "    # Select the model to use\n",
    "    #model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "    # Define the user prompt template\n",
    "    user_template = \"\"\"Provide weather and city information for {city}, {country}.\"\"\"\n",
    "\n",
    "    # Format the user prompt with the specified city and country\n",
    "    user_prompt = user_template.format(city=\"New York\", country=\"United States\")\n",
    "\n",
    "    # Define Pydantic models for the tool inputs\n",
    "\n",
    "    class WeatherRequest(BaseModel):\n",
    "        city: str = Field(..., description=\"Name of the city to get weather information for\")\n",
    "        country: str = Field(..., description=\"Country where the city is located\")\n",
    "\n",
    "    class CityInfoRequest(BaseModel):\n",
    "        city: str = Field(..., description=\"Name of the city to get information for\")\n",
    "        country: str = Field(..., description=\"Country where the city is located\")\n",
    "\n",
    "    # List of Pydantic models to register\n",
    "    tool_schemas = [WeatherRequest, CityInfoRequest]\n",
    "\n",
    "    # Define tool functions to process the Pydantic models\n",
    "\n",
    "    def get_weather_info(request: WeatherRequest) -> dict[str, Any]:\n",
    "        logger.info(f\"Getting weather information for {request.city}, {request.country}...\")\n",
    "        # In a real scenario, this function would make an API call to a weather service\n",
    "        # For this example, we'll return mock data based on the input\n",
    "        if request.city.lower() == \"new york\" and request.country.lower() == \"united states\":\n",
    "            return {\n",
    "                \"temperature\": \"75 degrees Fahrenheit\",\n",
    "                \"condition\": \"Partly cloudy\",\n",
    "                \"humidity\": 65\n",
    "            }\n",
    "        elif request.city.lower() == \"london\" and request.country.lower() == \"united kingdom\":\n",
    "            return {\n",
    "                \"temperature\": 15.0,\n",
    "                \"condition\": \"Rainy\",\n",
    "                \"humidity\": 80\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Weather information not available for {request.city}, {request.country}\")\n",
    "\n",
    "    def get_city_info(request: CityInfoRequest) -> dict[str, Any]:\n",
    "        logger.info(f\"Getting city information for {request.city}, {request.country}...\")\n",
    "        # In a real scenario, this function would query a database or make an API call\n",
    "        # For this example, we'll return mock data based on the input\n",
    "        if request.city.lower() == \"new york\" and request.country.lower() == \"united states\":\n",
    "            return {\n",
    "                \"population\": 8_419_000,\n",
    "                \"country\": \"United States\",\n",
    "                \"timezone\": \"Eastern Time Zone (ET)\"\n",
    "            }\n",
    "        elif request.city.lower() == \"london\" and request.country.lower() == \"united kingdom\":\n",
    "            return {\n",
    "                \"population\": 8_982_000,\n",
    "                \"country\": \"United Kingdom\",\n",
    "                \"timezone\": \"British Summer Time (BST)\"\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"City information not available for {request.city}, {request.country}\")\n",
    "\n",
    "    tool_processors = {\n",
    "        \"WeatherRequest\": get_weather_info,\n",
    "        \"CityInfoRequest\": get_city_info,\n",
    "    }\n",
    "\n",
    "    response_generator = call_model_with_tools(\n",
    "        model_id=model_id,\n",
    "        user_prompt=user_prompt,\n",
    "        tool_schemas=tool_schemas,\n",
    "        tool_processors=tool_processors,\n",
    "        use_streaming=True,\n",
    "        max_retries=3,\n",
    "        log_level=\"INFO\",\n",
    "    )\n",
    "\n",
    "    output_text, response_data = process_generator_output(response_generator)\n",
    "\n",
    "    print_response_data(response_data)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather and City Information Example (Real APIs)\n",
    "\n",
    "This example shows the assistant using two real API tools:\n",
    "\n",
    "1. WeatherRequest: Gets actual current weather data for a city and country using a weather API.\n",
    "2. CityInfoRequest: Retrieves real background information about a location using Wikipedia's API.\n",
    "\n",
    "The assistant needs to decide which tools to use based on the user's question and integrate real-time data from both APIs into a single, informative response. This example interacts with live data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import wikipediaapi\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main function to demonstrate calling a model with weather and location info tools.\"\"\"\n",
    "\n",
    "    # Select the model to use\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "    # Define the user prompt template\n",
    "    user_template = \"\"\"Provide information about the current weather and some background for {location}. \n",
    "    \"\"\"\n",
    "\n",
    "    # Format the user prompt with an example location\n",
    "    user_prompt = user_template.format(location=\"New York\")\n",
    "\n",
    "    class LocationNotFoundError(Exception):\n",
    "        \"\"\"Raised when a location isn't found.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_current_weather(location: str) -> dict[str, str] | None:\n",
    "        \"\"\"Returns the current weather for the requested location using wttr.in.\"\"\"\n",
    "        logger.info(f\"Getting weather information for {location}...\")\n",
    "        try:\n",
    "            response = requests.get(f\"https://wttr.in/{location}?format=%t+%C&u\")\n",
    "            if response.status_code == 200:\n",
    "                weather_data = response.text.strip().split()\n",
    "                return {\"temperature\": weather_data[0], \"condition\": \" \".join(weather_data[1:])}\n",
    "            else:\n",
    "                raise LocationNotFoundError(f\"Location {location} not found.\")\n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"Error fetching weather data: {e}\")\n",
    "            raise LocationNotFoundError(f\"Location {location} not found.\")\n",
    "\n",
    "    def get_location_info(location: str, char_limit: int = 2000) -> dict[str, str] | None:\n",
    "        \"\"\"Returns background information for the requested location using Wikipedia.\"\"\"\n",
    "        logger.info(f\"Getting background information for {location}...\")\n",
    "        headers = {\n",
    "            'User-Agent': 'LocationInfoBot/1.0 (https://example.org/locationbot/; locationbot@example.org)'\n",
    "        }\n",
    "\n",
    "        wiki_wiki = wikipediaapi.Wikipedia(\n",
    "            language='en',\n",
    "            extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
    "            user_agent=headers['User-Agent']\n",
    "        )\n",
    "        page = wiki_wiki.page(location)\n",
    "\n",
    "        if page.exists():\n",
    "            full_text = page.text[:char_limit]\n",
    "            return {\"full_text\": full_text}\n",
    "        else:\n",
    "            raise LocationNotFoundError(f\"Location {location} not found on Wikipedia.\")\n",
    "\n",
    "    class WeatherRequest(BaseModel):\n",
    "        \"\"\"Model for requesting the current weather from a location.\"\"\"\n",
    "        location: str = Field(..., description=\"The name of the location for which you want the current weather. Example locations are New York and London.\")\n",
    "\n",
    "    class LocationInfoRequest(BaseModel):\n",
    "        \"\"\"Model for requesting background information about a location.\"\"\"\n",
    "        location: str = Field(..., description=\"The name of the location for which you want the background information. Example locations are New York and London.\")\n",
    "        char_limit: int = Field(2000, description=\"The maximum number of characters to return from the Wikipedia page. Default is 2000.\")\n",
    "\n",
    "\n",
    "    # List of Pydantic models to register\n",
    "    tool_schemas = [WeatherRequest, LocationInfoRequest]\n",
    "\n",
    "    # Define tool functions to process the Pydantic models\n",
    "    def process_weather_request(request: WeatherRequest) -> dict[str, Any]:\n",
    "        try:\n",
    "            weather_data = get_current_weather(request.location)\n",
    "            return {\"weather\": weather_data}\n",
    "        except LocationNotFoundError as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def process_location_info_request(request: LocationInfoRequest) -> dict[str, Any]:\n",
    "        try:\n",
    "            location_info = get_location_info(request.location, request.char_limit)\n",
    "            return {\"info\": location_info}\n",
    "        except LocationNotFoundError as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    tool_processors = {\n",
    "        \"WeatherRequest\": process_weather_request,\n",
    "        \"LocationInfoRequest\": process_location_info_request,\n",
    "    }\n",
    "\n",
    "    response_dict = call_model_with_tools(\n",
    "        model_id=model_id,\n",
    "        user_prompt=user_prompt,\n",
    "        tool_schemas=tool_schemas,\n",
    "        tool_processors=tool_processors,\n",
    "        use_streaming=False,\n",
    "        max_retries=3,\n",
    "    )\n",
    "\n",
    "    for item in response_dict[\"tool_calls\"]:\n",
    "        if isinstance(item, WeatherRequest):\n",
    "            print(f\"WeatherRequest: {item}\")\n",
    "        elif isinstance(item, LocationInfoRequest):\n",
    "            print(f\"LocationInfoRequest: {item}\")\n",
    "        else:  # If response_type is 'messages'\n",
    "            print(f\"Message: {item}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip Planning Example\n",
    "\n",
    "This example involves planning a 3-day trip using three tools:\n",
    "\n",
    "1. WeatherForecast: Gets weather predictions for the destination.\n",
    "2. TouristAttractions: Finds popular attractions in the area.\n",
    "3. FlightSearch: Looks for available flights.\n",
    "\n",
    "The assistant must recognize that all three tools are needed for a complete trip plan. It has to decide the order to use the tools, gather the necessary information, and then create a coherent plan that includes weather, attractions, and travel details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\" Main function to demonstrate calling a model with multiple tools for trip planning. \"\"\"\n",
    "\n",
    "    # Select the model to use\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "    # Define the user prompt template\n",
    "    user_template = \"\"\"Plan a 3-day trip to {city}, {country} for next month. \n",
    "    Provide information about the weather, top attractions, and available flights from {origin_city}. \n",
    "    \"\"\"\n",
    "\n",
    "    # Format the user prompt with example values\n",
    "    user_prompt = user_template.format(\n",
    "        city=\"Paris\",\n",
    "        country=\"France\",\n",
    "        origin_city=\"New York\"\n",
    "    )\n",
    "\n",
    "    # Define Pydantic models for the tool inputs\n",
    "\n",
    "    class WeatherForecast(BaseModel):\n",
    "        city: str = Field(..., description=\"Name of the city\")\n",
    "        country: str = Field(..., description=\"Country where the city is located\")\n",
    "        start_date: str = Field(..., description=\"Start date of the forecast (YYYY-MM-DD)\")\n",
    "        end_date: str = Field(..., description=\"End date of the forecast (YYYY-MM-DD)\")\n",
    "\n",
    "    class TouristAttractions(BaseModel):\n",
    "        city: str = Field(..., description=\"Name of the city\")\n",
    "        country: str = Field(..., description=\"Country where the city is located\")\n",
    "        num_attractions: int = Field(5, description=\"Number of top attractions to return\")\n",
    "\n",
    "    class FlightSearch(BaseModel):\n",
    "        origin: str = Field(..., description=\"Origin city\")\n",
    "        destination: str = Field(..., description=\"Destination city\")\n",
    "        date: str = Field(..., description=\"Date of travel (YYYY-MM-DD)\")\n",
    "\n",
    "    # List of Pydantic models to register\n",
    "    tool_schemas = [WeatherForecast, TouristAttractions, FlightSearch]\n",
    "\n",
    "    # Define tool functions to process the Pydantic models\n",
    "\n",
    "    def get_weather_forecast(request: WeatherForecast) -> Dict[str, Any]:\n",
    "        logger.info(f\"Getting weather forecast for {request.city}, {request.country}...\")\n",
    "        if request.city.lower() == \"paris\" and request.country.lower() == \"france\":\n",
    "            start_date = datetime.strptime(request.start_date, \"%Y-%m-%d\")\n",
    "            forecast = []\n",
    "            for i in range(3):\n",
    "                date = start_date + timedelta(days=i)\n",
    "                forecast.append({\n",
    "                    \"date\": date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"temperature\": 20 + i,  # Simulated temperature increase\n",
    "                    \"condition\": \"Partly cloudy\" if i % 2 == 0 else \"Sunny\"\n",
    "                })\n",
    "            return {\"forecast\": forecast}\n",
    "        else:\n",
    "            return {\"error\": f\"Weather forecast not available for {request.city}, {request.country}\"}\n",
    "\n",
    "    def get_tourist_attractions(request: TouristAttractions) -> Dict[str, Any]:\n",
    "        logger.info(f\"Getting top attractions for {request.city}, {request.country}...\")\n",
    "        if request.city.lower() == \"paris\" and request.country.lower() == \"france\":\n",
    "            attractions = [\n",
    "                \"Eiffel Tower\",\n",
    "                \"Louvre Museum\",\n",
    "                \"Notre-Dame Cathedral\",\n",
    "                \"Arc de Triomphe\",\n",
    "                \"Champs-ÃlysÃ©es\"\n",
    "            ]\n",
    "            return {\"attractions\": attractions[:request.num_attractions]}\n",
    "        else:\n",
    "            return {\"error\": f\"Tourist attractions not available for {request.city}, {request.country}\"}\n",
    "\n",
    "    def search_flights(request: FlightSearch) -> Dict[str, Any]:\n",
    "        logger.info(f\"Searching flights from {request.origin} to {request.destination}...\")\n",
    "        if request.origin.lower() == \"new york\" and request.destination.lower() == \"paris\":\n",
    "            return {\n",
    "                \"flights\": [\n",
    "                    {\"airline\": \"Air France\", \"departure\": \"08:00\", \"arrival\": \"21:00\", \"price\": 800},\n",
    "                    {\"airline\": \"Delta\", \"departure\": \"10:30\", \"arrival\": \"23:30\", \"price\": 750},\n",
    "                    {\"airline\": \"United\", \"departure\": \"14:00\", \"arrival\": \"03:00\", \"price\": 700}\n",
    "                ]\n",
    "            }\n",
    "        else:\n",
    "            return {\"error\": f\"Flight search not available for {request.origin} to {request.destination}\"}\n",
    "\n",
    "    tool_processors = {\n",
    "        \"WeatherForecast\": get_weather_forecast,\n",
    "        \"TouristAttractions\": get_tourist_attractions,\n",
    "        \"FlightSearch\": search_flights,\n",
    "    }\n",
    "\n",
    "    response_dict = call_model_with_tools(\n",
    "        model_id=model_id,\n",
    "        user_prompt=user_prompt,\n",
    "        tool_schemas=tool_schemas,\n",
    "        tool_processors=tool_processors,\n",
    "        use_streaming=True,\n",
    "        max_retries=3,\n",
    "    )\n",
    "\n",
    "    print(f\"response_dict.keys(): {response_dict.keys()}\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workout Program Example\n",
    "\n",
    "This example demonstrates a different approach to using Pydantic models with the assistant:\n",
    "\n",
    "1. WorkoutPlan: A complex, nested Pydantic model defining a multi-week workout schedule.\n",
    "\n",
    "Unlike previous examples where Pydantic models specified input parameters for tools, here the Pydantic model defines the structure of the desired output. The assistant must generate a complete workout plan that conforms to this predefined structure.\n",
    "\n",
    "This complex nested Pydantic structure serves as a rigorous test of the model's ability to adhere to a provided toolSpec. It's important to note that Anthropic LLMs are not guaranteed to produce output that exactly matches the input schema. As a result, Pydantic validation may sometimes fail. When this happens, it triggers a retry mechanism that adds the Pydantic validation failure reason to the message before attempting again.\n",
    "\n",
    "Note: The docstring of the WorkoutPlan class and detailed descriptions for each field are critical in this example. These comprehensive descriptions significantly improve the likelihood that the LLM will return a response with the correct syntax. Clear and thorough documentation of the expected structure helps guide the model in generating appropriately formatted output.\n",
    "\n",
    "Practical consideration: In practice, this example will often fail initially and trigger the retry logic if the number of weeks required in the WorkoutPlan is set to a large number (like over 6). This illustrates the challenges of generating extensive, structured content that consistently meets all specified requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\" Main function to demonstrate calling a model with tools. \n",
    "\n",
    "    This is where you would configure several things, including:\n",
    "    - The model ID to use\n",
    "    - The user prompt\n",
    "    - The Pydantic models to register\n",
    "    - Tool functions to process the Pydantic models\n",
    "    - The response type (tools or messages)\n",
    "    - Whether to use streaming or not\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Select the model to use\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "    # Define the user prompt template\n",
    "    user_template = \"\"\"Create a workout program for the below person:\n",
    "\n",
    "    Age: {age}\n",
    "    Weight: {weight}\n",
    "    Gender: {gender}\n",
    "    Height: {height}\n",
    "\n",
    "    I want the program to focus on cardio and weight loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the user prompt with example values\n",
    "    user_prompt = user_template.format(\n",
    "        age=60,\n",
    "        weight=\"220 lbs\",\n",
    "        gender=\"male\",\n",
    "        height=\"6'3\"\n",
    "    )\n",
    "\n",
    "    # Define Pydantic models to make available to the model\n",
    "\n",
    "    # NOTE: CURRENTLY THERE IS NO GUARANTEE THAT CLAUDE MODELS WILL ADHERE EXACTLY TO THE PROVIDED TOOL SPEC\n",
    "    # THIS MAY CAUSE PARSING BACK TO THE PYDANTIC OBJECT TO FAIL\n",
    "    # This is why Pydantic validation and retry logic is important\n",
    "    # https://github.com/anthropics/anthropic-sdk-python/issues/619\n",
    "\n",
    "    class WorkoutDay(BaseModel):\n",
    "        description: str = Field(..., description=\"Description of the workout for this day.\")\n",
    "        duration: int = Field(..., ge=0, description=\"Duration of the workout in minutes. Every day requires a duration, even rest days (use 0).\")\n",
    "\n",
    "    class WorkoutWeek(BaseModel):\n",
    "        days: list[WorkoutDay] = Field(..., min_items=7, max_items=7, description=\"List of exactly 7 workout days.\")\n",
    "\n",
    "    class WorkoutPlan(BaseModel):\n",
    "        \"\"\"Model for a 90-day workout plan.\"\"\"\n",
    "        weeks: list[WorkoutWeek] = Field(..., min_items=13, max_items=13, \n",
    "                                         description=\"List of exactly 13 workout weeks. It's crucial that all 13 weeks are included!\")\n",
    "\n",
    "        @property\n",
    "        def total_duration(self) -> int:\n",
    "            \"\"\"Calculate the total duration of the workout plan in minutes.\"\"\"\n",
    "            return sum(day.duration for week in self.weeks for day in week.days)\n",
    "\n",
    "        @property\n",
    "        def rest_days(self) -> int:\n",
    "            \"\"\"Calculate the number of rest days (days with duration 0) in the plan.\"\"\"\n",
    "            return sum(1 for week in self.weeks for day in week.days if day.duration == 0)\n",
    "\n",
    "        def get_week(self, week_number: int) -> WorkoutWeek:\n",
    "            \"\"\"Get a specific week's workout plan.\"\"\"\n",
    "            if 1 <= week_number <= 13:\n",
    "                return self.weeks[week_number - 1]\n",
    "            raise ValueError(\"Week number must be between 1 and 13.\")\n",
    "\n",
    "        def get_day(self, day_number: int) -> WorkoutDay:\n",
    "            \"\"\"Get a specific day's workout plan.\"\"\"\n",
    "            if 1 <= day_number <= 90:\n",
    "                week_index = (day_number - 1) // 7\n",
    "                day_index = (day_number - 1) % 7\n",
    "                return self.weeks[week_index].days[day_index]\n",
    "            raise ValueError(\"Day number must be between 1 and 90.\")\n",
    "\n",
    "    # List of Pydantic models to register\n",
    "    tool_schemas = [WorkoutPlan]\n",
    "\n",
    "    # Define tool functions to process the Pydantic models\n",
    "\n",
    "    def process_workout_plan(workout_plan: WorkoutPlan) -> dict[str, Any]:\n",
    "        logger.info(\"Processing workout plan...\")\n",
    "        # Convert the Pydantic model to a dictionary (since the model itself is not JSON serializable)\n",
    "        # Optionally, you could also perform additional processing here\n",
    "        return workout_plan.model_dump()\n",
    "\n",
    "    tool_processors = {\n",
    "        \"WorkoutPlan\": process_workout_plan,\n",
    "    }\n",
    "\n",
    "    response_dict = call_model_with_tools(\n",
    "        model_id=model_id,\n",
    "        user_prompt=user_prompt,\n",
    "        tool_schemas=tool_schemas,\n",
    "        tool_processors=tool_processors,  # In this case, `None` would use the default processing function (and have the same result)\n",
    "        use_streaming=True,\n",
    "        invoke_limit=None,  # Set to `1` to stop after generating a schedule, but before generating an explanation of the schedule\n",
    "        max_retries=4,\n",
    "    )\n",
    "\n",
    "    def display_workout_plan(workout_plan: WorkoutPlan) -> None:\n",
    "        \"\"\"\n",
    "        Convert a WorkoutPlan instance to a pandas DataFrame and display it.\n",
    "\n",
    "        Args:\n",
    "        workout_plan (WorkoutPlan): An instance of the WorkoutPlan class\n",
    "\n",
    "        Returns:\n",
    "        None: Displays the DataFrame in the notebook\n",
    "        \"\"\"\n",
    "        # Create a list to hold all the data\n",
    "        data = []\n",
    "\n",
    "        # Iterate through each week and day\n",
    "        for week_num, week in enumerate(workout_plan.weeks, start=1):\n",
    "            for day_num, day in enumerate(week.days, start=1):\n",
    "                data.append({\n",
    "                    'Week': week_num,\n",
    "                    'Day': day_num,\n",
    "                    'Description': day.description,\n",
    "                    'Duration (minutes)': day.duration\n",
    "                })\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Set multi-level index\n",
    "        df.set_index(['Week', 'Day'], inplace=True)\n",
    "\n",
    "        # Style the DataFrame\n",
    "        styled_df = df.style.set_properties(**{\n",
    "            'background-color': 'lightyellow',\n",
    "            'border-color': 'black',\n",
    "            'border-style': 'solid',\n",
    "            'border-width': '1px',\n",
    "            'text-align': 'left'\n",
    "        })\n",
    "\n",
    "        # Highlight rest days (where duration is 0)\n",
    "        styled_df = styled_df.applymap(lambda x: 'background-color: lightgreen' if x == 0 else '', subset=['Duration (minutes)'])\n",
    "\n",
    "        # Display the styled DataFrame\n",
    "        display(styled_df)\n",
    "\n",
    "\n",
    "    for item in response_dict[\"tool_calls\"]:\n",
    "        # If the item is a WorkoutPlan object, we can access its properties\n",
    "        if isinstance(item, WorkoutPlan):\n",
    "            print(f\"Total duration: {item.total_duration} minutes\")\n",
    "            print(f\"Number of rest days: {item.rest_days}\")\n",
    "            print(f\"Day 5: {item.get_day(5)}\")\n",
    "\n",
    "            # Display the workout plan using a DataFrame\n",
    "            display_workout_plan(item)\n",
    "        # Otherwise if we chose to return the messages directly, we can print them\n",
    "        else:\n",
    "            print(item)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Database Financial Health Analysis Example\n",
    "\n",
    "This example demonstrates an LLM agent analyzing a customer's financial health based on a given ID. The agent must sequentially use two tools: CustomerInfoRequest to query a SQL database for customer data, and TransactionInfoRequest to fetch transaction history from a NoSQL database. Key challenges include constructing correct queries for each database, recognizing the need to use both tools in sequence, and determining when sufficient information has been gathered to provide a comprehensive answer. The agent then analyzes the retrieved data to synthesize a summary of the customer's financial status with recommendations. This process tests the agent's ability to follow a multi-step process, integrate information from multiple sources, and generate insights based on the compiled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "def main() -> None:\n",
    "\n",
    "    # Select the model to use\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "    # Define the user prompt template\n",
    "    user_template = \"\"\"Analyze the financial health of the customer with ID {customer_id}. \n",
    "    Consider their credit score and recent transaction history. \n",
    "    Provide a summary of their financial status and any recommendations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the user prompt with an example customer ID\n",
    "    user_prompt = user_template.format(customer_id=\"12345\")\n",
    "\n",
    "\n",
    "    # Simulated SQL database function (PostgreSQL dialect)\n",
    "    def query_customer_db(query: str) -> list[dict[str, Any]]:\n",
    "        \"\"\"Simulates querying a PostgreSQL database for customer information.\"\"\"\n",
    "        mock_data: dict[str, list[dict[str, Any]]] = {\n",
    "            \"SELECT customer_id, name, credit_score FROM customers WHERE customer_id = '12345'\": \n",
    "                [{\"customer_id\": \"12345\", \"name\": \"John Doe\", \"credit_score\": 720}],\n",
    "            \"SELECT customer_id, name, credit_score FROM customers WHERE customer_id = '67890'\":\n",
    "                [{\"customer_id\": \"67890\", \"name\": \"Jane Smith\", \"credit_score\": 680}]\n",
    "        }\n",
    "        result = mock_data.get(query, [])\n",
    "        if not result:\n",
    "            logger.warning(f\"No data found for query: {query}\")\n",
    "        return result\n",
    "\n",
    "    # Simulated NoSQL database function (MongoDB-like API)\n",
    "    def query_transaction_db(collection: str, query: dict[str, Any]) -> list[dict[str, Any]]:\n",
    "        \"\"\"Simulates querying a MongoDB-like database for transaction information.\"\"\"\n",
    "        mock_data: dict[str, list[dict[str, Any]]] = {\n",
    "            \"transactions\": [\n",
    "                {\"customer_id\": \"12345\", \"amount\": 1000, \"date\": \"2023-05-01\", \"type\": \"deposit\"},\n",
    "                {\"customer_id\": \"12345\", \"amount\": 500, \"date\": \"2023-05-15\", \"type\": \"withdrawal\"},\n",
    "                {\"customer_id\": \"67890\", \"amount\": 2000, \"date\": \"2023-05-02\", \"type\": \"deposit\"},\n",
    "                {\"customer_id\": \"67890\", \"amount\": 1500, \"date\": \"2023-05-20\", \"type\": \"withdrawal\"}\n",
    "            ]\n",
    "        }\n",
    "        result = [item for item in mock_data.get(collection, []) if all(item.get(k) == v for k, v in query.items())]\n",
    "        if not result:\n",
    "            logger.warning(f\"No data found for collection: {collection} with query: {query}\")\n",
    "        return result\n",
    "\n",
    "    class CustomerInfoRequest(BaseModel):\n",
    "        \"\"\"\n",
    "        Model for requesting customer information from the SQL database.\n",
    "\n",
    "        This model is used to construct a SQL query to fetch customer data from a PostgreSQL database.\n",
    "        The query should select the customer_id, name, and credit_score columns from the customers table.\n",
    "        \"\"\"\n",
    "        sql_query: str = Field(\n",
    "            ...,\n",
    "            description=\"SQL query to fetch customer information. Use PostgreSQL dialect. \"\n",
    "                        \"The query should select customer_id, name, and credit_score from the customers table. \"\n",
    "                        \"Example: SELECT customer_id, name, credit_score FROM customers WHERE customer_id = '12345'\"\n",
    "        )\n",
    "\n",
    "    class TransactionInfoRequest(BaseModel):\n",
    "        \"\"\"\n",
    "        Model for requesting transaction information from the NoSQL database.\n",
    "\n",
    "        This model is used to construct a query for a MongoDB-like database to fetch transaction data.\n",
    "        The collection should be 'transactions' and the query should filter by the customer_id.\n",
    "        \"\"\"\n",
    "        collection: str = Field(\n",
    "            ...,\n",
    "            description=\"Name of the collection to query. Should be 'transactions'.\"\n",
    "        )\n",
    "        query: dict[str, Any] = Field(\n",
    "            ...,\n",
    "            description=\"Query parameters for fetching transaction data. \"\n",
    "                        \"Should be a dictionary with 'customer_id' as the key and the customer's ID as the value. \"\n",
    "                        \"Example: {'customer_id': '12345'}\"\n",
    "        )\n",
    "\n",
    "    # List of Pydantic models to register\n",
    "    tool_schemas = [CustomerInfoRequest, TransactionInfoRequest]\n",
    "\n",
    "    # Define tool functions to process the Pydantic models\n",
    "    def process_customer_info_request(request: CustomerInfoRequest) -> Dict[str, Any]:\n",
    "        logger.info(f\"Processing customer info request with SQL query: {request.sql_query}\")\n",
    "        try:\n",
    "            result = query_customer_db(request.sql_query)\n",
    "            return {\"customer_info\": result}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing customer info request: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def process_transaction_info_request(request: TransactionInfoRequest) -> Dict[str, Any]:\n",
    "        logger.info(f\"Processing transaction info request for collection '{request.collection}' with query: {request.query}\")\n",
    "        try:\n",
    "            result = query_transaction_db(request.collection, request.query)\n",
    "            return {\"transaction_info\": result}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing transaction info request: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    tool_processors = {\n",
    "        \"CustomerInfoRequest\": process_customer_info_request,\n",
    "        \"TransactionInfoRequest\": process_transaction_info_request,\n",
    "    }\n",
    "\n",
    "    response_dict = call_model_with_tools(\n",
    "        model_id=model_id,\n",
    "        user_prompt=user_prompt,\n",
    "        tool_schemas=tool_schemas,\n",
    "        tool_processors=tool_processors,\n",
    "        use_streaming=False,\n",
    "        max_retries=3,\n",
    "    )\n",
    "\n",
    "    for item in response_dict[\"tool_calls\"]:\n",
    "        if isinstance(item, CustomerInfoRequest):\n",
    "            print(f\"CustomerInfoRequest: {item}\")\n",
    "        elif isinstance(item, TransactionInfoRequest):\n",
    "            print(f\"TransactionInfoRequest: {item}\")\n",
    "        else:\n",
    "            print(item)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longer Workout Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from typing import List, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "def main() -> None:\n",
    "    start_time = time.time()\n",
    "\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "    user_template = \"\"\"Create a workout program for the below person:\n",
    "\n",
    "    Age: {age}\n",
    "    Weight: {weight}\n",
    "    Gender: {gender}\n",
    "    Height: {height}\n",
    "\n",
    "    I want the program to focus on cardio and weight loss.\n",
    "\n",
    "    {continuation_prompt}\n",
    "\n",
    "    Please provide the next 30 days of the workout plan.\n",
    "    \"\"\"\n",
    "\n",
    "    user_info = {\n",
    "        \"age\": 60,\n",
    "        \"weight\": \"220 lbs\",\n",
    "        \"gender\": \"male\",\n",
    "        \"height\": \"6'3\"\n",
    "    }\n",
    "\n",
    "    class WorkoutDay(BaseModel):\n",
    "        description: str = Field(..., description=\"Description of the workout for this day.\")\n",
    "        duration: int = Field(..., ge=0, description=\"Duration of the workout in minutes. Every day requires a duration, even rest days (use 0).\")\n",
    "\n",
    "    class WorkoutMonth(BaseModel):\n",
    "        days: list[WorkoutDay] = Field(..., min_items=30, max_items=30, description=\"A list (not a string!) of exactly 30 workout days.\")\n",
    "        explanation: str = Field(..., description=\"An explanation of the workout plan from day 1 to end.\")\n",
    "\n",
    "        @property\n",
    "        def explanation(self) -> str:\n",
    "            return self.explanation\n",
    "\n",
    "    tool_schemas = [WorkoutMonth]\n",
    "\n",
    "    def process_workout_month(workout_month: WorkoutMonth) -> dict[str, Any]:\n",
    "        return workout_month.model_dump()\n",
    "\n",
    "    tool_processors = {\n",
    "        \"WorkoutMonth\": process_workout_month,\n",
    "    }\n",
    "\n",
    "    full_schedule = []\n",
    "\n",
    "    def display_workout_month(workout_month: WorkoutMonth, month_number: int) -> None:\n",
    "        data = []\n",
    "        for day_num, day in enumerate(workout_month.days, start=1):\n",
    "            data.append({\n",
    "                'Day': day_num + month_number * 30,\n",
    "                'Description': day.description,\n",
    "                'Duration (minutes)': day.duration\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df.set_index('Day', inplace=True)\n",
    "\n",
    "        styled_df = df.style.set_properties(**{\n",
    "            'background-color': 'lightyellow',\n",
    "            'border-color': 'black',\n",
    "            'border-style': 'solid',\n",
    "            'border-width': '1px',\n",
    "            'text-align': 'left'\n",
    "        })\n",
    "\n",
    "        styled_df = styled_df.applymap(lambda x: 'background-color: lightgreen' if x == 0 else '', subset=['Duration (minutes)'])\n",
    "\n",
    "        display(styled_df)\n",
    "\n",
    "    for month in range(3):\n",
    "        if month == 0:\n",
    "            continuation_prompt = \"\"\n",
    "        else:\n",
    "            previous_days = full_schedule[-30:]\n",
    "            continuation_prompt = \"Continue the workout plan based on the following previous 30 days:\\n\\n\"\n",
    "            for i, day in enumerate(previous_days, start=1):\n",
    "                continuation_prompt += f\"Day {i + (month-1)*30}: {day.description} ({day.duration} minutes)\\n\"\n",
    "\n",
    "        user_prompt = user_template.format(**user_info, continuation_prompt=continuation_prompt)\n",
    "\n",
    "        response_dict = call_model_with_tools(\n",
    "            model_id=model_id,\n",
    "            user_prompt=user_prompt,\n",
    "            tool_schemas=tool_schemas,\n",
    "            tool_processors=tool_processors,\n",
    "            use_streaming=True,\n",
    "            invoke_limit=1,\n",
    "            max_retries=4,\n",
    "            log_level=\"CRITICAL\",\n",
    "        )\n",
    "\n",
    "        for item in response_dict[\"tool_calls\"]:\n",
    "            if isinstance(item, WorkoutMonth):\n",
    "                full_schedule.extend(item.days)\n",
    "                display_workout_month(item, month)\n",
    "                print(item.explanation)\n",
    "\n",
    "    print(f\"Total duration: {sum(day.duration for day in full_schedule)} minutes\")\n",
    "    print(f\"Number of rest days: {sum(1 for day in full_schedule if day.duration == 0)}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time:.3f} seconds\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
